{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantization :**  \n",
        "Quantization in the context of fine-tuning Large Language Models (LLMs) refers to the process of converting the model's weights and, sometimes, activations from high-precision floating-point representations (e.g., 32-bit floats) to lower-precision formats (e.g., 8-bit integers). This approach can significantly reduce the memory footprint and computational requirements of LLMs, making them more efficient for deployment in various applications, especially on resource-constrained devices.  <br>  \n",
        "1. Benefits of Quantization:\n",
        "\n",
        "  - Reduced Memory Usage: Lower precision requires less memory, allowing larger\n",
        "    models or more instances to fit into the same hardware.\n",
        "  - Faster Inference: Integer arithmetic can be performed more quickly than\n",
        "    floating-point operations on many hardware accelerators, leading to faster model inference times.\n",
        "  - Lower Power Consumption: Reduced precision can also lead to lower power\n",
        "    requirements, which is crucial for mobile and edge devices.\n",
        "2. Types of Quantization:\n",
        "\n",
        "  - **Post-Training Quantization:** This technique is applied after the model has been trained. The model weights are converted to lower precision without further training. This method is simple but might lead to some accuracy loss.\n",
        "      - Example: Quantizing a GPT model after training from FP32 (32-bit floating-point) to INT8 (8-bit integer) to make it smaller and faster without retraining\n",
        "  - **Quantization-Aware Training (QAT):** This involves incorporating quantization into the training process. During training, the model learns to compensate for the reduced precision, which can help maintain performance. This method typically yields better results than post-training quantization.\n",
        "      - Example: Training a BERT model with 8-bit precision during forward and backward passes, so it learns to handle quantization noise during training.\n",
        "3. Quantization Techniques:\n",
        "\n",
        "  - Weight Quantization: Converting the model's weights to lower precision (e.g., from 32-bit floats to 8-bit integers).\n",
        "  - Activation Quantization: Similar conversion applied to the activations of neurons during inference.\n",
        "  \n",
        "  - Dynamic vs. Static Quantization: In dynamic quantization, the scale and zero-point are calculated on-the-fly during inference. In static quantization, these parameters are computed beforehand and remain constant.\n",
        ""
      ],
      "metadata": {
        "id": "Zq-sjuCBW23x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "Tutorial part\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "6i31bKRObRNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Full Precision / half precision  \n",
        "2. Calibration - model Quantization\n",
        "3. Modes of Quantization  \n",
        "  - Post-Training Quantization\n",
        "  - Quantization-aware -training"
      ],
      "metadata": {
        "id": "zCAH5h-EZ4_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defination :  \n",
        "**Quatization :**  \n",
        "conversation from higher memory format to a lower memory formate  <br>  \n",
        "**post-training Quatization :** It causes loss of data and loss of accuracy  \n",
        "**Quatization Aware training:** We will add more data and don't loss accuracy\n"
      ],
      "metadata": {
        "id": "egl7-YcmbbCF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6ta5yj-Vuwf"
      },
      "outputs": [],
      "source": []
    }
  ]
}